{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Regression Neural Network.ipynb","provenance":[{"file_id":"1sRj8K8B8h5oFVTIaj7Wnhz_V_kNrijyw","timestamp":1605492034510},{"file_id":"1nI8QW4bSDH2x9ZprChO9T0ZfllhWHRrn","timestamp":1605191898287},{"file_id":"12sGMT82ZDy9oKnhf61VfXFhCkP80o8iL","timestamp":1605156472552}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"SNsF8TF-f3nR","executionInfo":{"status":"ok","timestamp":1605488118941,"user_tz":300,"elapsed":10693,"user":{"displayName":"Bernardo Macías","photoUrl":"","userId":"07961460086734177546"}},"outputId":"05c231a4-869a-4774-a361-e5ae715438d4","colab":{"base_uri":"https://localhost:8080/"}},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","from tensorflow import keras\n","import tensorflow_addons as tfa\n","\n","# first neural network with keras tutorial\n","from numpy import loadtxt\n","from keras.models import Sequential\n","from keras.layers import Dense\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from datetime import datetime\n","import numpy as np\n","from numpy.testing import assert_allclose\n","from keras.models import Sequential, load_model\n","from keras.layers import LSTM, Dropout, Dense\n","from keras.callbacks import ModelCheckpoint\n","\n","import math"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RHhPSYwSyM_I"},"source":["# Data load and pre-processing"]},{"cell_type":"code","metadata":{"id":"_M5UAHfOYFqr"},"source":["rutaCarp = '/content/drive/My Drive/2020 Proyecto DS4A'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgPAgpZMg6FJ"},"source":["ofertas = pd.read_csv(rutaCarp+'/data_no_outliers.csv', delimiter=\";\") "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y4QHlft5ygC7"},"source":["All the categorical data is marked in the data frame"]},{"cell_type":"code","metadata":{"id":"TBAkF7hLzoTF"},"source":["cat_data = ['oft_tipo_inmueble',\n","            'oft_tipo_norma_juridica',\n","            'loccodigo',\n","            'suelo',\n","            'actividad',\n","            'tratamiento_urb',\n","            'topografia',\n","            'serpub',\n","            'serpub_tipo',\n","            'serpub_especif',\n","            'via',\n","            'clase_via',\n","            'estado_via',\n","            'influencia_via',\n","            'actividad_economica',\n","            'actividad_economica_tipo',\n","            'tipo_segun_actividad',\n","            'cp_terr_ar',\n","            'estrato']\n","for cd in cat_data:\n","    ofertas[cd] = ofertas[cd].astype('category')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7IPEq2RZy5YY"},"source":["We filter the variables that will be used in the model"]},{"cell_type":"code","metadata":{"id":"44NW5upRj7kW"},"source":["cols_analisis = ['x', 'y',\n","                 \n","                 'd_park', 'd_highway', 'd_bikeway', 'd_ssf', 'd_mus', 'd_lib', 'd_sitp',\n","                 'd_tm', 'd_p_tm', 'd_gy', 'd_ies', 'd_bom', 'd_col', 'd_ips',\n","\n","                 'oft_tipo_inmueble', 'oic_area_terreno',\n","                 'oia_cant_garajes',\n","                 'loccodigo', 'actividad_economica', 'estrato']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wyLNzsZ3zLPM"},"source":["The data is split into input (X) and output (y) variables"]},{"cell_type":"code","metadata":{"id":"FtUWCNwiiRvS"},"source":["X = pd.get_dummies(ofertas[cols_analisis])\n","numInp = len(X.columns)\n","Y = ofertas[[\"log_vfventa2020\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b-dRXzOFzR2P"},"source":["The previous data frames are sub-divided into train and test"]},{"cell_type":"code","metadata":{"id":"TTMuCW1ZzSTs"},"source":["x_train, x_test, y_train, y_test = train_test_split( X, Y, test_size=0.3, random_state=123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O6vNtgykznzH"},"source":["# Data Balancing and Size reduction\n","About half of the data corresponds to middle-class socioeconomic strata, so 1000 random properties are selected from each stratum to ensure that the model works properly in all strata."]},{"cell_type":"code","metadata":{"id":"xGKOubeR9PF5","executionInfo":{"status":"ok","timestamp":1605488119035,"user_tz":300,"elapsed":10727,"user":{"displayName":"Bernardo Macías","photoUrl":"","userId":"07961460086734177546"}},"outputId":"9765cdff-a7e0-4436-e6dc-5a833aa5493d","colab":{"base_uri":"https://localhost:8080/"}},"source":["#se balancean los estratos\n","var_estratos = ['estrato_1.0','estrato_2.0','estrato_3.0','estrato_4.0','estrato_5.0','estrato_6.0']\n","min_datos = min(x_train[var_estratos].sum())\n","redond = math.floor(min_datos/1000)*1000\n","min_datos"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1574"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"5oAEb7qZGl6O"},"source":["np.random.seed(1234) # a seed makes the analysis reproducible\n","                     # so everyone will get the same results\n","\n","df_est_x = x_train[x_train['estrato_1.0']==1]\n","df_est_y = y_train[x_train['estrato_1.0']==1]\n","\n","ndata = len(df_est_x)\n","idx_entrenamiento = np.random.choice(range(ndata),redond,replace=False)\n","idx_resto  = np.asarray(list(set(range(ndata)) - set(idx_entrenamiento)))\n","\n","train_x_entr= df_est_x.iloc[idx_entrenamiento] # the training data set\n","train_x_otro= df_est_x.iloc[idx_resto]  # the test data set\n","\n","train_y_entr= df_est_y.iloc[idx_entrenamiento] # the training data set\n","train_y_otro= df_est_y.iloc[idx_resto]  # the test data set\n","\n","var_estratos_2 = ['estrato_2.0','estrato_3.0','estrato_4.0','estrato_5.0','estrato_6.0']\n","for estrati in var_estratos_2:\n","  df_est_x = x_train[x_train[estrati]==1]\n","  df_est_y = y_train[x_train[estrati]==1]\n","  \n","  ndata = len(df_est_x)\n","  idx_entrenamiento = np.random.choice(range(ndata),redond,replace=False)\n","  idx_resto  = np.asarray(list(set(range(ndata)) - set(idx_entrenamiento)))\n","  \n","  train_x_entr_estr= df_est_x.iloc[idx_entrenamiento] # the training data set\n","  train_x_rest_estr= df_est_x.iloc[idx_resto]  # the test data set\n","\n","  train_y_entr_estr= df_est_y.iloc[idx_entrenamiento] # the training data set\n","  train_y_rest_estr= df_est_y.iloc[idx_resto]  # the test data set\n","\n","  train_x_entr = pd.concat([train_x_entr, train_x_entr_estr], ignore_index=True)\n","  train_x_otro = pd.concat([train_x_otro, train_x_rest_estr], ignore_index=True)\n","  train_y_entr = pd.concat([train_y_entr, train_y_entr_estr], ignore_index=True)\n","  train_y_otro = pd.concat([train_y_otro, train_y_rest_estr], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kv3ZvdGI1G8X"},"source":["# Data Transformation\n","In addition to the transformed dependent variable (using the logarithm function), the explanatory variables are normalized with their average and deviation."]},{"cell_type":"code","metadata":{"id":"dbtURj6iilfo"},"source":["def normalize(train, test):\n","\n","    mean = np.mean(train, axis=0)\n","    std = np.std(train, axis=0)+0.000001\n","\n","    X_train = (train - mean) / std\n","    X_test = (test - mean) /std\n","    return X_train, X_test\n","\n","x_train_2, x_train_2_otro = normalize(train_x_entr, train_x_otro)\n","y_train_2, y_train_2_otro = normalize(train_y_entr, train_y_otro)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJzEJn6_1lF5"},"source":["# Neural Network Configurations \n","Next, we parameterize the characteristics of the neural network."]},{"cell_type":"code","metadata":{"id":"7X6fMq6JL9KU"},"source":["constante = 50\n","capa1 = int(numInp*constante)\n","capa2 = int(numInp*constante)\n","capa3 = int(numInp*constante)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ok_ZVn9mT2gn"},"source":["now = datetime.now()\n","date_time = now.strftime(\"%Y%m%d-%H:%M v7\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2osOYuX2U19"},"source":["To be able to compare the results of the Neural Network with the other models, we define the R Square metric. "]},{"cell_type":"code","metadata":{"id":"ONj66eL9HiyL"},"source":["def r_square(y_true, y_pred):\n","    from keras import backend as K\n","    SS_res =  K.sum(K.square(y_true - y_pred)) \n","    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n","    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FdI38zus2eT7"},"source":["# Neural Network Initialization\n","Next, we compile and train the neural network with the previous configurations."]},{"cell_type":"code","metadata":{"id":"8qs8ODSqFVGh","executionInfo":{"status":"ok","timestamp":1605388429188,"user_tz":300,"elapsed":73820,"user":{"displayName":"Uno Dos","photoUrl":"","userId":"08922581512112579438"}},"outputId":"7ded0f27-85f2-4137-faaa-5b55c17afb7c","colab":{"base_uri":"https://localhost:8080/"}},"source":["#Model initialization\n","model = Sequential()\n","model.add(Dense(capa1, input_dim=numInp, activation='linear'))\n","model.add(Dense(capa2, activation='linear'))\n","model.add(Dense(capa3, activation='linear'))\n","model.add(Dense(capa3, activation='linear'))\n","model.add(Dense(1, activation='linear'))\n","model.compile(loss='mean_squared_error', optimizer='adam', metrics=[r_square])\n","\n","# define the checkpoint\n","\n","filepath = rutaCarp+'/logs/'+date_time+' NN Model.h5'\n","checkpoint = ModelCheckpoint(filepath, monitor='r_square', save_freq=96*2, verbose=1, save_best_only=True, mode='max')\n","time_stopping_callback = tfa.callbacks.TimeStopping(seconds=85*60, verbose=1)\n","  \n","callbacks_list = [checkpoint,time_stopping_callback]\n","\n","# fit the model\n","model.fit(x_train_2, y_train_2, epochs=10, batch_size=50, verbose=1, validation_split=0.2, callbacks=callbacks_list)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n"," 1/96 [..............................] - ETA: 0s - loss: 0.7846 - r_square: 0.1052WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0045s vs `on_train_batch_end` time: 0.0187s). Check your callbacks.\n","96/96 [==============================] - 2s 20ms/step - loss: 4542.7490 - r_square: -4561.1289 - val_loss: 18.4877 - val_r_square: -67.3811\n","Epoch 2/10\n","95/96 [============================>.] - ETA: 0s - loss: 8.7518 - r_square: -9.5082\n","Epoch 00002: r_square improved from -inf to -9.40807, saving model to /content/drive/My Drive/2020 Proyecto DS4A/logs/20201114-21:12 v7 NN Model.h5\n","96/96 [==============================] - 4s 45ms/step - loss: 8.6691 - r_square: -9.4081 - val_loss: 0.8056 - val_r_square: -2.1406\n","Epoch 3/10\n","96/96 [==============================] - 2s 17ms/step - loss: 0.9328 - r_square: -0.0104 - val_loss: 0.7771 - val_r_square: -2.0284\n","Epoch 4/10\n","94/96 [============================>.] - ETA: 0s - loss: 0.8719 - r_square: 0.0575\n","Epoch 00004: r_square improved from -9.40807 to 0.05878, saving model to /content/drive/My Drive/2020 Proyecto DS4A/logs/20201114-21:12 v7 NN Model.h5\n","96/96 [==============================] - 4s 46ms/step - loss: 0.8669 - r_square: 0.0588 - val_loss: 0.8029 - val_r_square: -2.1217\n","Epoch 5/10\n","96/96 [==============================] - 2s 17ms/step - loss: 0.8286 - r_square: 0.1136 - val_loss: 0.6515 - val_r_square: -1.4607\n","Epoch 6/10\n","92/96 [===========================>..] - ETA: 0s - loss: 0.7382 - r_square: 0.2174\n","Epoch 00006: r_square improved from 0.05878 to 0.22188, saving model to /content/drive/My Drive/2020 Proyecto DS4A/logs/20201114-21:12 v7 NN Model.h5\n","96/96 [==============================] - 15s 158ms/step - loss: 0.7303 - r_square: 0.2219 - val_loss: 0.5941 - val_r_square: -1.2563\n","Epoch 7/10\n","96/96 [==============================] - 2s 18ms/step - loss: 0.7278 - r_square: 0.2272 - val_loss: 0.5579 - val_r_square: -1.1219\n","Epoch 8/10\n","92/96 [===========================>..] - ETA: 0s - loss: 0.6725 - r_square: 0.2842\n","Epoch 00008: r_square improved from 0.22188 to 0.28521, saving model to /content/drive/My Drive/2020 Proyecto DS4A/logs/20201114-21:12 v7 NN Model.h5\n","96/96 [==============================] - 17s 180ms/step - loss: 0.6721 - r_square: 0.2852 - val_loss: 0.6461 - val_r_square: -1.5473\n","Epoch 9/10\n","96/96 [==============================] - 2s 17ms/step - loss: 0.6741 - r_square: 0.2715 - val_loss: 0.4874 - val_r_square: -0.8266\n","Epoch 10/10\n","93/96 [============================>.] - ETA: 0s - loss: 0.6147 - r_square: 0.3290\n","Epoch 00010: r_square improved from 0.28521 to 0.32804, saving model to /content/drive/My Drive/2020 Proyecto DS4A/logs/20201114-21:12 v7 NN Model.h5\n","96/96 [==============================] - 23s 236ms/step - loss: 0.6184 - r_square: 0.3280 - val_loss: 0.4857 - val_r_square: -0.8178\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fa4ce0625c0>"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"lCxCfXyT2wcK"},"source":["# Neural Network Saving\n","Finally, we save the model along with some metrics to be able to compare it with another neural network compiled with different configurations"]},{"cell_type":"code","metadata":{"id":"B_femtKFWos4","executionInfo":{"status":"ok","timestamp":1605389619230,"user_tz":300,"elapsed":11070,"user":{"displayName":"Uno Dos","photoUrl":"","userId":"08922581512112579438"}},"outputId":"232d6e90-b141-4308-a91f-1bfbf3d9de84","colab":{"base_uri":"https://localhost:8080/"}},"source":["\n","new_model = load_model(filepath, custom_objects={'r_square':r_square})\n","\n","checkpoint = ModelCheckpoint(filepath, monitor='r_square', save_freq=96*2, verbose=1, save_best_only=True, mode='max')\n","time_stopping_callback = tfa.callbacks.TimeStopping(seconds=85*60, verbose=1)\n","  \n","callbacks_list = [checkpoint,time_stopping_callback]\n","history_callback=new_model.fit(x_train_2, y_train_2, epochs=2, verbose=1, batch_size=50,validation_split=0.2, callbacks=callbacks_list)\n","\n","loss_history = history_callback.history[\"r_square\"]\n","numpy_loss_history = np.array(loss_history)\n","np.savetxt(rutaCarp+'/logs/'+date_time+' log NN est r2.txt', numpy_loss_history, delimiter=\",\")\n","\n","loss_history = history_callback.history[\"loss\"]\n","numpy_loss_history = np.array(loss_history)\n","np.savetxt(rutaCarp+'/logs/'+date_time+' log NN est loss.txt', numpy_loss_history, delimiter=\",\")\n","\n","ruta= rutaCarp+'/logs/'+date_time+' log NN desc.txt'\n","with open(ruta, \"w\") as outfile:\n","    outfile.write(\"\\n\".join(cols_analisis))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/2\n"," 1/96 [..............................] - ETA: 0s - loss: 0.3426 - r_square: 0.7168WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0039s vs `on_train_batch_end` time: 0.0188s). Check your callbacks.\n","96/96 [==============================] - 2s 20ms/step - loss: 0.3229 - r_square: 0.6570 - val_loss: 0.4290 - val_r_square: -0.6681\n","Epoch 2/2\n","93/96 [============================>.] - ETA: 0s - loss: 0.3587 - r_square: 0.5899\n","Epoch 00002: r_square improved from -inf to 0.59523, saving model to /content/drive/My Drive/2020 Proyecto DS4A/logs/20201114-21:12 v7 NN Model temp.h5\n","96/96 [==============================] - 5s 51ms/step - loss: 0.3553 - r_square: 0.5952 - val_loss: 0.4783 - val_r_square: -0.8899\n"],"name":"stdout"}]}]}